{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectures 7 and 8: Building Autograd engine from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning outcomes\n",
    "1. Understanding Automated Differentiation Engines at a foundational level\n",
    "2. Operator Overloading in Objoct Oriented Programming\n",
    "3. Graph Representation\n",
    "4. Graph Traversal\n",
    "\n",
    "To Draw a painting representing the conciseness of the mathematics, the systematism of DSA and the abstraction of programming paradigms like OOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Restricting yourself to <font color=\"red\">Python's Standard Library</font>, build an <font color=\"red\">Autograd Engine</font> capable of estimating the gradients required to solve the following problem using gradient descent.\n",
    "<br><br>\n",
    "Find a point in $\\mathbb{R}^{2}$ with the least average Euclidean distance to a set of arbitrary points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously in the course\n",
    "from random import Random\n",
    "from math import ceil, sqrt\n",
    "\n",
    "def generate_pnts(N=1000):\n",
    "    rndm_obj = Random(x=5)\n",
    "    return [rndm_obj.uniform(a=0, b=1) for _ in range(N)],[rndm_obj.uniform(a=0, b=1) for _ in range(N)]\n",
    "\n",
    "def calc_grad_1(x_p, y_p, batch_x ,batch_y):\n",
    "  sum_x, sum_y = 0, 0\n",
    "  n = len(batch_x)\n",
    "  for x_i, y_i in zip(batch_x ,batch_y):\n",
    "    inv_sqrt = ((x_i - x_p) ** 2 + (y_i - y_p) ** 2) ** (-0.5)\n",
    "    sum_x += inv_sqrt * (x_i - x_p)\n",
    "    sum_y += inv_sqrt * (y_i - y_p)\n",
    "  return -sum_x/n, -sum_y/n\n",
    "\n",
    "def loss(x_p, y_p, batch_x ,batch_y):\n",
    "  return (1/len(batch_x))* sum([sqrt((x_i-x_p)**2+(y_i-y_p)**2) for x_i , y_i in zip(batch_x, batch_y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3277744397151291 -0.336282174741702\n"
     ]
    }
   ],
   "source": [
    "data_x, data_y = generate_pnts()\n",
    "\n",
    "dl_dx_1, dl_dy_1 = calc_grad_1(0.3,0.3,data_x,data_y)\n",
    "print(dl_dx_1,dl_dy_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4430528244756474"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss without tensors\n",
    "loss(0.3,0.3,data_x,data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as an example of Autograd engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "pnt = torch.tensor([0.3,0.3])\n",
    "pnt.requires_grad = True\n",
    "data_tnsr = torch.tensor([data_x,data_y])\n",
    "data_tnsr = data_tnsr.t()\n",
    "#each row, contains two points of the dataset\n",
    "print(data_tnsr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4431, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss with tensors\n",
    "loss_torch = torch.mean(torch.sqrt(((data_tnsr-pnt)**2).sum(dim=1)))\n",
    "loss_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it leaf, to retain grad\n",
    "pnt.retain_grad()\n",
    "#if no backward, no grad, it would be none\n",
    "loss_torch.backward()\n",
    "torch_grad = pnt.grad.data\n",
    "torch_grad\n",
    "pnt.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "loss torch:0.44305282831192017\n",
      "torch grad = tensor([-0.3278, -0.3363])\n",
      "iteration 1\n",
      "loss torch:0.44305282831192017\n",
      "torch grad = tensor([-0.6555, -0.6726])\n",
      "iteration 2\n",
      "loss torch:0.44305282831192017\n",
      "torch grad = tensor([-0.9833, -1.0088])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"iteration {i}\")\n",
    "    loss_torch = torch.mean(torch.sqrt(((data_tnsr-pnt)**2).sum(dim=1)))\n",
    "    print(f\"loss torch:{loss_torch}\")\n",
    "    loss_torch.backward()\n",
    "    print(f\"torch grad = {pnt.grad.data}\")\n",
    "pnt.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "loss torch:0.44305282831192017\n",
      "torch grad = tensor([-0.3278, -0.3363])\n",
      "iteration 1\n",
      "loss torch:0.44305282831192017\n",
      "torch grad = tensor([-0.3278, -0.3363])\n",
      "iteration 2\n",
      "loss torch:0.44305282831192017\n",
      "torch grad = tensor([-0.3278, -0.3363])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"iteration {i}\")\n",
    "    loss_torch = torch.mean(torch.sqrt(((data_tnsr-pnt)**2).sum(dim=1)))\n",
    "    print(f\"loss torch:{loss_torch}\")\n",
    "    loss_torch.backward()\n",
    "    print(f\"torch grad = {pnt.grad.data}\")\n",
    "    pnt.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Autograd From Scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why using a class? because it's the paradigm that keeps the state and the behaviour\n",
    "class comp_node:\n",
    "    def __init__(self,val,children=[], op = \"assign\"):\n",
    "        self.val = val # a place holder in the memory\n",
    "        self.children = children\n",
    "        self.grad = 0 # initialized with 0 because we do a (plus equal operation)\n",
    "        self.op = op \n",
    "        self.backward_prop = lambda : None\n",
    "    def __to_comp_node__(self, obj):\n",
    "        if not isinstance(obj, comp_node):\n",
    "            return comp_node(val = obj)\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def __sub__(self,other):\n",
    "        other = self.__to_comp_node__(other)\n",
    "        out = comp_node(val=self.val - other.val, children=[self,other], op = \"subtraction\")\n",
    "        def _backward_prop():\n",
    "            self.grad -= out.grad * (-1)\n",
    "            other.grad -= out.grad * (-1)\n",
    "        out.backward_prop = _backward_prop\n",
    "        return out\n",
    "    def __rsub__(self,other):\n",
    "        other = self.__to_comp_node__(other)\n",
    "       # out = comp_node(val=other.val - self.val, children=[self,other])\n",
    "        return other - self\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = self.__to_comp_node__(other)\n",
    "        out = comp_node(val=self.val + other.val, children=[self,other], op = \"addition\")\n",
    "        def _backward_prop():\n",
    "            self.grad += out.grad *1\n",
    "            other.grad += out.grad *1\n",
    "        out.backward_prop = _backward_prop\n",
    "        return out\n",
    "    def __radd__(self,other):\n",
    "        other = self.__to_comp_node__(other)\n",
    "       # out = comp_node(val=other.val - self.val, children=[self,other])\n",
    "        return other + self\n",
    "    def __mul__(self,other):\n",
    "        other = self.__to_comp_node__(other)\n",
    "        out = comp_node(val=self.val * other.val, children=[self,other], op = \"multplication\")\n",
    "        def __backward_prop():\n",
    "         self.grad += out.grad * other.val\n",
    "         other.grad += out.grad * self.val\n",
    "        out.backward_prop = __backward_prop \n",
    "        return out\n",
    "    def __rmul__(self,other):\n",
    "        other = self.__to_comp_node__(other)\n",
    "       # out = comp_node(val=other.val - self.val, children=[self,other])\n",
    "        return other * self\n",
    "    def __pow__(self, exponent):\n",
    "        if not isinstance(exponent, (int, float)):\n",
    "            raise ValueError(\"unsupported types\")\n",
    "        out = comp_node(val = self.val ** exponent, children=[self], op = f\"power {exponent}\")\n",
    "        def _backward_prop():\n",
    "          self.grad += out.grad * (exponent * self.val **(exponent-1))\n",
    "        out.backward_prop = _backward_prop\n",
    "        return out\n",
    "    def __eq__(self,other):\n",
    "        return self.val == other.val\n",
    "    def __repr__(self):\n",
    "        return f\"op: {self.op} | val: {self.val:0.5f} | number of children {len(self.children)} | gradient {self.grad}\"\n",
    "\n",
    "\n",
    "assert comp_node(val = 5 ).val == 5, \"assignment failed\"\n",
    "assert (comp_node(val = 5) - comp_node(val = 3)).val == 2, \"nodes subtraction overloading failed\"\n",
    "assert (comp_node(val = 5) - 3).val == 2, \"integer and node subtraction overloading failed\"\n",
    "#in rsub-> other - self \n",
    "assert ( 5 - comp_node(val =3)).val == 2, \"integer and node right subtraction overloading failed\"\n",
    "\n",
    "assert (comp_node(val = 5) + comp_node(val = 3)).val == 8, \"nodes addition overloading failed\"\n",
    "assert (comp_node(val = 5) + 3).val == 8, \"integer and node addition overloading failed\"\n",
    "#in radd-> other + self \n",
    "assert ( 5 + comp_node(val =3)).val == 8, \"integer and node right addidtion overloading failed\"\n",
    "assert (comp_node(val=25)**0.5).val == 5, \"node power int or float failed\"\n",
    "assert comp_node(val = 5)**2 == comp_node(val =25), \"nodes comparison failed\"\n",
    "assert (5 * comp_node(val =2)).val == 10, \"node muliplying failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 op: power 0.5 | val: 0.54721 | number of children 1 | gradient 1\n",
      "1 op: addition | val: 0.29944 | number of children 2 | gradient 0.9137222319490423\n",
      "2 op: power 2 | val: 0.10427 | number of children 1 | gradient 0.9137222319490423\n",
      "3 op: power 2 | val: 0.19518 | number of children 1 | gradient 0.9137222319490423\n",
      "4 op: subtraction | val: 0.32290 | number of children 2 | gradient 0.5900849147094943\n",
      "5 op: subtraction | val: 0.44179 | number of children 2 | gradient 0.8073411877467226\n",
      "6 op: assign | val: 0.30000 | number of children 0 | gradient 0.5900849147094943\n",
      "7 op: assign | val: 0.30000 | number of children 0 | gradient 0.8073411877467226\n"
     ]
    }
   ],
   "source": [
    "data_x, data_y = generate_pnts(N=1)\n",
    "x_p,y_p = comp_node(val=0.3), comp_node(val=0.3)\n",
    "\n",
    "def graph_loss(xp, yp, datax , datay):\n",
    "    #n = len(datax)\n",
    "    #l,M,gx,gy, Ix, Iy = 0,0,0,0,0,0\n",
    "    #for datax, datay in zip(datax, datay):\n",
    "    Ix = datax - xp\n",
    "    Iy = datay - yp\n",
    "    gx = Ix**2\n",
    "    gy = Iy**2\n",
    "    M = gx + gy\n",
    "    l = M ** 0.5\n",
    "    #l = l * (1/n)\n",
    "    return l, [l,M,gx,gy, Ix, Iy, xp, yp] # \n",
    "\n",
    "graph_loss_val, reverse_topo_order = graph_loss(x_p, y_p, data_x[0], data_y[0])\n",
    "reverse_topo_order[0].grad = 1\n",
    "\n",
    "for i, node in enumerate(reverse_topo_order):\n",
    "  node.backward_prop()\n",
    "  print(i, node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
